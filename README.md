# LLM Exploration:
Understanding LLM engineering and structure with GPT decoder-only architecture --- LLM course mlabonne

**Tokenization** : Splitting text into tokens (words/subwords)

**Attention Mechanism** : Self-attention and Scaled Dot-product Attention, model focus on different parts of input data for better performance

**Text Generation** : Generate output sequences with decoding strategies like greedy decoding, beam search, top-k sampling and nucleus sampling

**Extra NLP Exploration**:
SkimLiterature: categorizing abstract sentences with NLP --- Zero to Mastery TensorFlow course
