# LLM Exploration:
GPT decoder-only architecture with Inputs as Tokens and Outputs as Logits

**Tokenization** : Splitting text into tokens (words/subwords)

**Attention Mechanism** : Self-attention and Scaled Dot-product Attention, model focus on different parts of input data for better performance

**Text Generation** : Generate output sequences with decoding strategies like greedy decoding, beam search, top-k sampling and nucleus sampling

**Extra NLP Exploration**:
SkimLiterature: categorizing abstract sentences with NLP --- Zero to Mastery TensorFlow course
