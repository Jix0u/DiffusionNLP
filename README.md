# LLM Exploration:
Understanding LLM engineering and structure with GPT decoder-only architecture --- LLM course mlabonne

1. **LLM Decoding Strategies** : Generate output sequences with decoding strategies like greedy decoding, beam search, top-k sampling and nucleus sampling
2. **LLM Weight Quantization** : Compress model parameters and reduce memory footprint with zero-point and abs-max quantization


3. **LLM Fine-tuning** : Adapt pre-trained LLMs to specific tasks or domains with transfer learning.

4. **Transformers Architecture**  : Explore LLM from transformer architecture to deployment

## AbsMax Weight Quantization
<img width="890" alt="Screenshot 2024-05-20 at 12 32 30â€¯PM" src="https://github.com/Jix0u/LLMTransformers/assets/55889031/3fdcd35e-b92b-4da3-b179-33dad363e2b0">

## Extra NLP Exploration:
1. **SkimLiterature**: categorizing abstract sentences with NLP --- Zero to Mastery TensorFlow course

2. **NLP Model Experimentation**: exploring recurrent neural networks RNNS, and convolutional neural networks CNNs to classify text

## References
The concepts and methodologies explored in this repository draw inspiration from various sources, including research papers, online courses, and community contributions.

## Usage
To get started with the LLM exploration and NLP projects, refer to the respective Jupyter notebooks provided in this repository.

